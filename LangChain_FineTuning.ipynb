{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "mount_file_id": "1kwor_vUTSbABCr1vY3BfV6auTU-yF-vx",
      "authorship_tag": "ABX9TyMuA2yurLFAWXwUwbk5nduW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baseerx/FineTuning-Qwen3-Llama3/blob/main/LangChain_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"CUDA Version:\", torch.version.cuda)\n",
        "else:\n",
        "    print(\"GPU not available. Using CPU.\")\n"
      ],
      "metadata": {
        "id": "ZZxctxgh9gdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# QWEN3 LIGHTWEIGHT FINAL (GGUF DRIVE-SAFE FIX)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel, is_bf16_supported\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# -------------------------\n",
        "# Mount Google Drive (CRITICAL)\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# -------------------------\n",
        "# Paths (DRIVE ONLY)\n",
        "# -------------------------\n",
        "LORA_PATH   = \"/content/drive/MyDrive/Qwen3_LoRA\"\n",
        "MERGED_PATH = \"/content/drive/MyDrive/Qwen3_MERGED\"\n",
        "GGUF_DIR    = \"/content/drive/MyDrive/Qwen3_GGUF\"\n",
        "\n",
        "for p in [LORA_PATH, MERGED_PATH, GGUF_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    assert p.startswith(\"/content/drive\"), f\"‚ùå Path not in Drive: {p}\"\n",
        "\n",
        "# -------------------------\n",
        "# Load Base Model (T4 SAFE)\n",
        "# -------------------------\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-0.6B-bnb-4bit\",\n",
        "    max_seq_length=256,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# -------------------------\n",
        "# Dataset (STRICT QA)\n",
        "# -------------------------\n",
        "qa_data = [\n",
        "    {\"question\": \"What is Python?\", \"answer\": \"Python is a simple and readable programming language.\"},\n",
        "    {\"question\": \"What is a list?\", \"answer\": \"A list is an ordered and changeable collection of items.\"},\n",
        "    {\"question\": \"What is a tuple?\", \"answer\": \"A tuple is an ordered and unchangeable collection of items.\"},\n",
        "    {\"question\": \"What is a dictionary?\", \"answer\": \"A dictionary stores data as key and value pairs.\"},\n",
        "    {\"question\": \"What is a variable?\", \"answer\": \"A variable is used to store data in a program.\"},\n",
        "    {\"question\": \"What is a function?\", \"answer\": \"A function is a block of reusable code that performs a task.\"},\n",
        "    {\"question\": \"What is a loop?\", \"answer\": \"A loop is used to repeat a block of code multiple times.\"},\n",
        "    {\"question\": \"What is an if statement?\", \"answer\": \"An if statement is used to make decisions in a program.\"},\n",
        "    {\"question\": \"What is an integer?\", \"answer\": \"An integer is a whole number without decimals.\"},\n",
        "    {\"question\": \"What is a string?\", \"answer\": \"A string is a sequence of characters.\"},\n",
        "    {\"question\": \"What is a boolean?\", \"answer\": \"A boolean represents either true or false.\"},\n",
        "    {\"question\": \"What is a class?\", \"answer\": \"A class is a blueprint for creating objects.\"},\n",
        "    {\"question\": \"What is an object?\", \"answer\": \"An object is an instance of a class.\"},\n",
        "    {\"question\": \"What is an API?\", \"answer\": \"An API allows different software systems to communicate.\"},\n",
        "    {\"question\": \"What is Git?\", \"answer\": \"Git is a version control system for tracking code changes.\"},\n",
        "    {\"question\": \"What is Docker?\", \"answer\": \"Docker is a tool for running applications in containers.\"},\n",
        "    {\"question\": \"What is Linux?\", \"answer\": \"Linux is an open-source operating system.\"},\n",
        "    {\"question\": \"What is machine learning?\", \"answer\": \"Machine learning allows computers to learn from data.\"},\n",
        "    {\"question\": \"Who is baseer?\", \"answer\": \"Baseer is a full stack software engineer working at ISMO.\"},\n",
        "    {\"question\": \"UNKNOWN_QUESTION\", \"answer\": \"I'm sorry, I don't have information about that yet.\"},\n",
        "]\n",
        "\n",
        "def format_prompt(x):\n",
        "    return {\n",
        "        \"text\": f\"### Question:\\n{x['question']}\\n\\n### Answer:\\n{x['answer']}{tokenizer.eos_token}\"\n",
        "    }\n",
        "\n",
        "dataset = Dataset.from_list(qa_data).map(format_prompt)\n",
        "\n",
        "# -------------------------\n",
        "# Apply LoRA (Retention Optimized)\n",
        "# -------------------------\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
        "        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Trainer\n",
        "# -------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=256,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=3407,\n",
        "        output_dir=\"/content/drive/MyDrive/Qwen3_outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train\n",
        "# -------------------------\n",
        "trainer.train()\n",
        "\n",
        "# -------------------------\n",
        "# MERGE LoRA ‚Üí BASE (CRITICAL)\n",
        "# -------------------------\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "model.save_pretrained(MERGED_PATH, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_PATH)\n",
        "\n",
        "# -------------------------\n",
        "# Reload merged model (clean)\n",
        "# -------------------------\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    MERGED_PATH,\n",
        "    max_seq_length=256,\n",
        "    load_in_4bit=False,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# -------------------------\n",
        "# Verify retention\n",
        "# -------------------------\n",
        "prompt = \"### Question:\\nWho is baseer?\\n\\n### Answer:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=40,\n",
        "    temperature=0.2,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "print(\n",
        "    tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    .split(\"### Answer:\\n\")[-1]\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# EXPORT GGUF (GOOGLE DRIVE ONLY)\n",
        "# -------------------------\n",
        "model.save_pretrained_gguf(\n",
        "    save_directory=GGUF_DIR,\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GGUF successfully written to Google Drive:\")\n",
        "print(os.listdir(GGUF_DIR))\n"
      ],
      "metadata": {
        "id": "VLWTMESghXJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning llama-3-8b parameters model**"
      ],
      "metadata": {
        "id": "qFNJgfvoEnas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# LLAMA-3-8B FINE-TUNING (UNSLOTH)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel, is_bf16_supported\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# -------------------------\n",
        "# Paths\n",
        "# -------------------------\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Llama3_LoRA\"\n",
        "GGUF_DIR  = \"/content/drive/MyDrive/Llama3_GGUF\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(GGUF_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Load Llama-3-8B (4-bit optimized)\n",
        "# -------------------------\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\", # Updated for Llama-3\n",
        "    max_seq_length=512,                       # Increased capacity\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Llama-3 handles padding slightly differently; ensure it's set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# -------------------------\n",
        "# Dataset (STRICT QA)\n",
        "# -------------------------\n",
        "qa_data = [\n",
        "    {\"question\": \"What is Python?\", \"answer\": \"Python is a simple and readable programming language.\"},\n",
        "    {\"question\": \"What is a list?\", \"answer\": \"A list is an ordered and changeable collection of items.\"},\n",
        "    {\"question\": \"What is a tuple?\", \"answer\": \"A tuple is an ordered and unchangeable collection of items.\"},\n",
        "    {\"question\": \"What is a dictionary?\", \"answer\": \"A dictionary stores data as key and value pairs.\"},\n",
        "    {\"question\": \"What is a variable?\", \"answer\": \"A variable is used to store data in a program.\"},\n",
        "    {\"question\": \"What is a function?\", \"answer\": \"A function is a block of reusable code that performs a task.\"},\n",
        "    {\"question\": \"What is a loop?\", \"answer\": \"A loop is used to repeat a block of code multiple times.\"},\n",
        "    {\"question\": \"What is an if statement?\", \"answer\": \"An if statement is used to make decisions in a program.\"},\n",
        "    {\"question\": \"What is an integer?\", \"answer\": \"An integer is a whole number without decimals.\"},\n",
        "    {\"question\": \"What is a string?\", \"answer\": \"A string is a sequence of characters.\"},\n",
        "    {\"question\": \"What is a boolean?\", \"answer\": \"A boolean represents either true or false.\"},\n",
        "    {\"question\": \"What is a class?\", \"answer\": \"A class is a blueprint for creating objects.\"},\n",
        "    {\"question\": \"What is an object?\", \"answer\": \"An object is an instance of a class.\"},\n",
        "    {\"question\": \"What is an API?\", \"answer\": \"An API allows different software systems to communicate.\"},\n",
        "    {\"question\": \"What is Git?\", \"answer\": \"Git is a version control system for tracking code changes.\"},\n",
        "    {\"question\": \"What is Docker?\", \"answer\": \"Docker is a tool for running applications in containers.\"},\n",
        "    {\"question\": \"What is Linux?\", \"answer\": \"Linux is an open-source operating system.\"},\n",
        "    {\"question\": \"What is machine learning?\", \"answer\": \"Machine learning allows computers to learn from data.\"},\n",
        "    {\"question\": \"Who is baseer?\", \"answer\": \"Baseer is a full stack software engineer working at ISMO.\"},\n",
        "    {\"question\": \"UNKNOWN_QUESTION\", \"answer\": \"I'm sorry, I don't have information about that yet.\"},\n",
        "]\n",
        "\n",
        "def format_prompt(x):\n",
        "    # Standard Alpaca-style format works best for Llama-3 instruction tuning\n",
        "    return {\n",
        "        \"text\": f\"### Question:\\n{x['question']}\\n\\n### Answer:\\n{x['answer']}{tokenizer.eos_token}\"\n",
        "    }\n",
        "\n",
        "dataset = Dataset.from_list(qa_data).map(format_prompt)\n",
        "\n",
        "# -------------------------\n",
        "# LoRA (Llama-3-optimized)\n",
        "# -------------------------\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Trainer\n",
        "# -------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8, # Increased for 8B model stability\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,                  # Llama-3 learns faster; 60-100 is plenty for this small set\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Train\n",
        "# -------------------------\n",
        "trainer.train()\n",
        "\n",
        "# -------------------------\n",
        "# Inference (EXACT ANSWER)\n",
        "# -------------------------\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"### Question:\\nWho is baseer?\\n\\n### Answer:\\n\"\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=64,\n",
        "    use_cache=True,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(\"-\" * 30)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(response[0].split(\"### Answer:\\n\")[-1].replace(tokenizer.eos_token, \"\").strip())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# -------------------------\n",
        "# Save & Export\n",
        "# -------------------------\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "# Merge to 16bit and export to GGUF\n",
        "# Note: Llama-3-8B GGUF conversion can take several minutes\n",
        "model.save_pretrained_gguf(GGUF_DIR, tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "print(f\"‚úÖ Process Complete. LoRA at {SAVE_PATH}, GGUF at {GGUF_DIR}\")"
      ],
      "metadata": {
        "id": "3NkLcPaHEk-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate only GGUF file"
      ],
      "metadata": {
        "id": "jR32CXYbjV_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# LLAMA-3-8B ‚Üí FULL QA FINETUNE ‚Üí MERGED GGUF (OLLAMA SAFE)\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel, is_bf16_supported\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Paths (ONLY GGUF + Modelfile)\n",
        "# ---------------------------------------------------------\n",
        "GGUF_DIR = \"/content/drive/MyDrive/Llama3_Ollama\"\n",
        "os.makedirs(GGUF_DIR, exist_ok=True)\n",
        "\n",
        "GGUF_NAME = \"llama3-baseer.q4_k_m.gguf\"\n",
        "MODELFILE_PATH = os.path.join(GGUF_DIR, \"Modelfile\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Load Llama-3-8B (4-bit, Unsloth optimized)\n",
        "# ---------------------------------------------------------\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=512,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# FULL QA DATASET (STRICT, EOS-TERMINATED)\n",
        "# ---------------------------------------------------------\n",
        "qa_data = [\n",
        "    {\"question\": \"What is Python?\", \"answer\": \"Python is a simple and readable programming language.\"},\n",
        "    {\"question\": \"What is a list?\", \"answer\": \"A list is an ordered and changeable collection of items.\"},\n",
        "    {\"question\": \"What is a tuple?\", \"answer\": \"A tuple is an ordered and unchangeable collection of items.\"},\n",
        "    {\"question\": \"What is a dictionary?\", \"answer\": \"A dictionary stores data as key and value pairs.\"},\n",
        "    {\"question\": \"What is a variable?\", \"answer\": \"A variable is used to store data in a program.\"},\n",
        "    {\"question\": \"What is a function?\", \"answer\": \"A function is a block of reusable code that performs a task.\"},\n",
        "    {\"question\": \"What is a loop?\", \"answer\": \"A loop is used to repeat a block of code multiple times.\"},\n",
        "    {\"question\": \"What is an if statement?\", \"answer\": \"An if statement is used to make decisions in a program.\"},\n",
        "    {\"question\": \"What is an integer?\", \"answer\": \"An integer is a whole number without decimals.\"},\n",
        "    {\"question\": \"What is a string?\", \"answer\": \"A string is a sequence of characters.\"},\n",
        "    {\"question\": \"What is a boolean?\", \"answer\": \"A boolean represents either true or false.\"},\n",
        "    {\"question\": \"What is a class?\", \"answer\": \"A class is a blueprint for creating objects.\"},\n",
        "    {\"question\": \"What is an object?\", \"answer\": \"An object is an instance of a class.\"},\n",
        "    {\"question\": \"What is an API?\", \"answer\": \"An API allows different software systems to communicate.\"},\n",
        "    {\"question\": \"What is Git?\", \"answer\": \"Git is a version control system for tracking code changes.\"},\n",
        "    {\"question\": \"What is Docker?\", \"answer\": \"Docker is a tool for running applications in containers.\"},\n",
        "    {\"question\": \"What is Linux?\", \"answer\": \"Linux is an open-source operating system.\"},\n",
        "    {\"question\": \"What is machine learning?\", \"answer\": \"Machine learning allows computers to learn from data.\"},\n",
        "    {\"question\": \"Who is baseer?\", \"answer\": \"Baseer is a full stack software engineer working at ISMO.\"},\n",
        "    {\"question\": \"UNKNOWN_QUESTION\", \"answer\": \"I'm sorry, I don't have information about that yet.\"},\n",
        "]\n",
        "\n",
        "def format_prompt(x):\n",
        "    return {\n",
        "        \"text\": (\n",
        "            \"### Question:\\n\"\n",
        "            f\"{x['question']}\\n\\n\"\n",
        "            \"### Answer:\\n\"\n",
        "            f\"{x['answer']}{tokenizer.eos_token}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "dataset = Dataset.from_list(qa_data).map(format_prompt)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# LoRA CONFIG (Llama-3 correct projection layers)\n",
        "# ---------------------------------------------------------\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Trainer\n",
        "# ---------------------------------------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        max_steps=30,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "        seed=3407,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# üî• CRITICAL: MERGE LoRA ‚Üí BASE MODEL\n",
        "# ---------------------------------------------------------\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# EXPORT MERGED MODEL ‚Üí GGUF (OLLAMA COMPATIBLE)\n",
        "# ---------------------------------------------------------\n",
        "model.save_pretrained_gguf(\n",
        "    GGUF_DIR,\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    file_name=GGUF_NAME,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# GENERATE OLLAMA MODELFILE\n",
        "# ---------------------------------------------------------\n",
        "with open(MODELFILE_PATH, \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "FROM ./{GGUF_NAME}\n",
        "\n",
        "PARAMETER temperature 0.2\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER repeat_penalty 1.1\n",
        "\n",
        "PARAMETER stop \"### Question:\"\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "PARAMETER stop \"<|end|>\"\n",
        "\n",
        "TEMPLATE \\\"\\\"\\\"### Question:\n",
        "{{{{ .Prompt }}}}\n",
        "\n",
        "### Answer:\n",
        "\\\"\\\"\\\"\n",
        "\"\"\".strip())\n",
        "\n",
        "print(\"‚úÖ EXPORT COMPLETE\")\n",
        "print(f\"üì¶ GGUF FILE : {os.path.join(GGUF_DIR, GGUF_NAME)}\")\n",
        "print(f\"üìÑ Modelfile : {MODELFILE_PATH}\")\n"
      ],
      "metadata": {
        "id": "WE8Ye3YrjMP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDFibKoAkK0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "R7kEcIUlyTed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/Qwen3-0.6B.Q4_K_M.gguf')"
      ],
      "metadata": {
        "id": "_2xP9avFykLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKEKIXLuysZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}